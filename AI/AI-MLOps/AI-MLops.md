up: [[AI-MLops]] ; see [[MLOps-flow]]
# MLOps 
![[AI-MLops#< AI-MLops MLOps >]]

##### AI software Stack
, ИИ софтверный стек-->  [[ML-sw-stack.canvas]]

### MLOps info

###### \* 13  ML Operations (mlsysbook.ai)
[13  ML Operations](https://mlsysbook.ai/contents/ops/ops.html)
###### \* Что такое MLOps? Самый подробный текст про работу с ML-системами, который вы найдете в интернете
2022 https://habr.com/ru/companies/selectel/articles/703460/



### LLM (web) UI
User Services/Interfaces for running LLMs locally (??)
LLM UI - GUI, webUI, CLI ; LLM (web) UI

OpenWebUI, Oogabooga (Text generation web UI), GPT4All, 
LibreChat, YakGPT, и еще много. 


jan.ai, 

chat client only, use SillyTavern instead
use LibreChat instead.
xxxx as OpenAI API client

##### Oogabooga (Text generation web UI)
text-generation-webui
![[oogabooga#^tldr]]
   --> Подробнее: [[oogabooga|oogabooga (Text generation web UI)]]

##### OpenWebUI
(used with [[#Ollama]])  // [[openwebui]]

##### GPT4All
[[GPT4All]] -  запускалка для разных моделей

##### LibreChat


##### YakGPT
https://github.com/yakGPT/yakGPT

##### jan.ai   (BIG)
<font color="#ffff00">by defaut GUI (not WEB)</font> , есть docker (для режима сервера?)
[[Jan.ai]]

##### LM Studio (BIG)(as chat)
[[#LM Studio (as model server)]]


### Remote AI APIs
like Groq and OpenRouter
OpenAI API

### Model servers
up: [[AI-concept#Model server (LLM Model server)|AI -> Model server (LLM Model server)]]  ; MLOps model server, Сервера моделей, Platforms for running LLMs locally, ака Запускалки,
Могут иметь свои сервисы/демоны
https://www.axelmendoza.com/posts/model-server/
https://www.axelmendoza.com/posts/model-server/#what-is-a-model-serving-runtime

Что такое MLOps? ... (article habr) --> [>>](https://habr.com/ru/companies/selectel/articles/703460/#:~:text=%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B9%20Serving%20Engine%29%2C-,Model%20Serving,-%2C%20%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B9%20%D0%B2%20%D1%80%D0%B0%D0%BC%D0%BA%D0%B0%D1%85)  -->

##### Ollama
[[Ollama]]    platform for running LLMs on your local machine.
ollama uses llama.cpp under the hood
llama.cpp  <font color="#ffff00">llama.cpp</font>
By default, Ollama saves its models in the ~/. ollama/models directory, which contains both model blobs and manifests.

##### LM Studio (as model server)
Ollama focuses on simplicity and ease of use, whereas LM Studio provides a richer feature set and a larger model library.
https://lmstudio.ai/
see: https://www.gpu-mart.com/blog/ollama-vs-lm-studio



### AI/LLM frameworks
#### Transformers ( HuggingFace )
framework ; [[hf-transformers]]

#### Huginn
(<font color="#ffff00">ruby</font>)  open-source platform for building agents that perform automated tasks online   https://github.com/huginn/huginn
google:// Huginn agent framework


#### langchain4j
(<font color="#ffff00">java</font>)  --> langchain4j
https://docs.langchain4j.dev/intro/
langchain4j // 
https://docs.langchain4j.dev/integrations/language-models/ollama/
интегрируется с ollama

### Misc AI

##### whisper
[[whisper-mlops]]  ( AI / .. /[[Wisper-AI]]  )

### Model files formats
|extentions
.pt or . pth - PyTorch Model
.gguf - Hugging Face's Transformers format file



#### Tech / sysadm / Devops
[[GPU_passthrough]]

--------





