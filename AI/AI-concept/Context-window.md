---
aliases:
  - Context window 
  - Контекстное окно
  - Окно контекста
up:
  - [[AI-concept]]
---
up:  [[Context-window]]

# Context window
Aliases: Контекстное окно, Окно контекста

##### \*  Оценка LLM с большим окном контекста
Sivchenko_translate 2 авг 2023 https://habr.com/ru/articles/752062/
MTS AI  .... свои фундаментальные языковые модели. ...  получилось достичь уровня gpt-4 на собственном ограниченном датасете большого контекста.



##### \*  Как сделать контекстное окно на 100K в большой языковой модели: обо всех фокусах в одном посте
Sivchenko_translate   2 авг 2023  https://habr.com/ru/articles/752062/

в статье рассмотрены приёмы, позволяющие ускорить обучение больших языковых моделей (LLM) и нарастить в них вывод (inference). Для этого нужно использовать большое контекстное окно, в котором умещается до 100K входных токенов. Вот эти приёмы: ALiBi с подмешиванием в вектор позиции слова в последовательности (positional embedding), разреженное внимание (Sparse Attention), мгновенное внимание (Flash Attention),



#### Context window size

2023 <font color="#ffff00">65K</font> , <font color="#ffff00">100K</font>   ,  GPT-4 <font color="#00b050">32K</font> токенов. опенсорс LLM <font color="#00b050">2K</font>

Недавно прошло несколько анонсов по поводу новых больших языковых моделей (LLM), настроенных на приём исключительно крупного контекстного окна, целых 65K токенов (MPT-7B-StoryWriter-65k+ от MosaicML) или даже 100K токенов (описано в статье Introducing 100K Context Windows от Antropic)
...
Для сравнения: современная модель GPT-4 может работать, получая контекст длиной 32K входных токенов. Большинство же опенсорсных моделей LLM работают с контекстным окном длиной 2K токенов.
see: [[#* Как сделать контекстное окно на 100K в большой языковой модели обо всех фокусах в одном посте]]
