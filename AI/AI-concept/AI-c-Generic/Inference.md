---
aliases:
  - Inference (Model Inference)
up:
  - [[AI-concept#Inference (Model Inference)]]
---
up:  [[AI-concept#Inference (Model Inference)]]

## Inference (Model Inference)
основное юзерское использование модели - запрашиваем ей запрос и получаем результат.
See also: [[Chaining#Chaining, Inference chaining, LLM chaining]]

> Model inference (or machine learning inference) is when a model makes predictions on new, unseen input data (inference data) and produces predictions as output that are consumed by a user or service.  https://www.hopsworks.ai/dictionary/model-inference

https://www.hopsworks.ai/dictionary/model-inference
Model inference requires a: 
ML model || inference data || an inference pipeline || prediction consumer. 
‍Batch Model Inference -> ...
‍Online Model Inference -> request-response network services, exposed as a REST or gRPC endpoint (authentication & access control )

###### Типы инференса ML моделей (article)
https://ml-system-design.ru/blog/inference/типы-инференса-ml-моделей 

###### inference pipeline
https://www.hopsworks.ai/dictionary/inference-pipeline




