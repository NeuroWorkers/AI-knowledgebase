---
aliases:
  - Mosaic MPT
  - MPT (MosaicML)
up:
  - [[AI#AI Models]]
---
up:  [[AI#AI Models]]
up: [[AI-centers#MosaicML]]
mosaic-MPT
## Mosaic MPT
NOTE: Author : [[AI-centers#MosaicML|MosaicML]]
https://www.mosaicml.com/

---
https://www.mosaicml.com/blog/mpt-7b

---
![[2023-06-07_23-21.webp]]

### Introducing MPT-7B: A New Standard for Open-Source, 
Commercially Usable LLMs
Introducing MPT-7B, the latest entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of ~$200k.

Large language models (LLMs) are changing the world, but for those outside well-resourced industry labs, it can be extremely difficult to train and deploy these models. This has led to a flurry of activity centered on open-source LLMs, such as the LLaMA series from Meta, the Pythia series from EleutherAI, the StableLM series from StabilityAI, and the OpenLLaMA model from Berkeley AI Research.  

![[photo_2023-05-11_16-11-17.opti.webp|300]] ![[photo_2023-05-11_16-11-27.opti.webp|340]]

---
### MPT-7B-StoryWriter (+)
model designed to read and write fictional stories
### MPT-7B-StoryWriter

https://huggingface.co/mosaicml/mpt-7b-storywriter

<b><font color="#00b0f0">MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.</font></b>

MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens. We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.


---

### Introducing MPT-7B: A New Open-Source LLM
https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html

An LLM Trained on 1T Tokens of Text and Code by MosaicML Foundation Series.
By Nisha Arya, KDnuggets on May 26, 2023 in Natural Language Processing

![[2023-06-08_02-32.webp|400]]


