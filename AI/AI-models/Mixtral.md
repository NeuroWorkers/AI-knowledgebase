# Mixtral
Mixtral-8x7B.

#### MoE – Mixture of Experts
Ее важная особенность, технология MoE – Mixture of Experts 
https://huggingface.co/blog/moe

see [[#MoE – Mixture of Experts – это не просто «склеить восемь сеток»]]

#### Orig 2023-12

новость на гитхабе
	https://github.com/huggingface/transformers/releases/tag/v4.36.0
	v4.36: Mixtral, Llava/BakLlava, SeamlessM4T v2, AMD ROCm, F.sdpa wide-spread support
	.
	Mixtral is the new open-source model from Mistral AI announced by the blogpost [Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/). The model has been proven to have comparable capabilities to Chat-GPT



>open source моделей много скачать можно на hugging face. обычно the bloke выкладывает. можно запустить на cpu с помощью llama.cpp в формате gguf. для это модели надо прилично памяти 32Г/40Г




https://www.linux.org.ru/news/opensource/17451901
Mistral, французский ИИ-стартап, который выложил свою последнюю крупную языковую модель в ссылке на торрент.
	В понедельник компания наконец дополнила свой первоначальный релиз блог-постом с подробностями о программе, которая просто называется Mixtral-8x7B. Согласно приведенным в посте бенчмаркам, алгоритм Mistral превосходит некоторых американских конкурентов, включая семейство Llama 2 от Meta и GPT-3.5 от OpenAI. Похоже, люди в интернете согласны с тем, что новый алгоритм Mistral довольно хорош.
	Бонус к этому — Mixtral-8x7B имеет открытый исходный код, в отличие от иронически названной OpenAI, которая держит свои последние LLM закрытыми, что вызвало определенное недовольство среди сообщества.


Основное достижение - придумали и показали, как склеить вместе восемь 7B сеток, натренированных на разных наборах данных. [Barracuda72](https://www.linux.org.ru/people/Barracuda72/profile)(15.12.23) [Ссылка](https://www.linux.org.ru/news/opensource/17451901?cid=17452875) (https://www.linux.org.ru/news/opensource/17451901)

-------------------

Забавно, что эта сеть устарела еще до выхода в публичный доступ.

Вообще, архитектура Transformers в том виде в котором она сейчас (с механизмом одиночного внимания) - уже две недели как устарела и я если честно немного удивлен что толком новостей об этом и нет. Разве что кроме вот этой [короткой заметки](https://medium.com/@jelkhoury880/what-is-mamba-845987734ffc)
Мне лень писать новость поэтому ограничусь комментарием. Первого декабря вышла [работа по архитектуре Mamba](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf), которая позволяет добавлять структурированные пространства состояний (SSM). Это позволяет использовать множественный контекст с линейной сложностью, а не квадратичной. Там и сокращение использования ресурсов при том же качестве модели по сравнению с Chat GPT и уверенная работа с последовательностями на порядки большей длины токенов чем в обучающих примерах и много чего еще.
[Obezyan](https://www.linux.org.ru/people/Obezyan/profile) (15.12.23) - [Ссылка](https://www.linux.org.ru/news/opensource/17451901?cid=17453158) 

<font color="#ffff00">see>></font> [[Mamba]]

----------------------------------

#### MoE – Mixture of Experts – это не просто «склеить восемь сеток»

MoE – Mixture of Experts – это не просто «склеить восемь сеток», это модель с точностью и качеством как у полносвязной модели аналогичного веса (40 млрд весов), при вычислительных затратах в режиме генерации как у модели на 7 миллиардов весов.

Еще это и в процессе обучения работает, но несколько меньший прирост даёт.

Обучается сеть как одно целое, а не раздельные части, не надо путать с мультимодальными моделями где есть основная модель и куча адаптеров которые переводят разные типы данных в понятны основно модели формат.

[timdorohin](https://www.linux.org.ru/people/timdorohin/profile) ★★★★ (16.12.23 17:15:16 GST)

- [Ссылка](https://www.linux.org.ru/news/opensource/17451901?cid=17454250)

---------------------------


на теории топологий нейросетей, читать про всякие свёртки и прочее прочее ибо нахлобучек сверху очень много и всяких разных , но суть, базовые принципы, фундамент едины что для чатГПТ что для наколенной домашней поделки.

А когда распознавалку полочек и кружков с цифорками сделаешь руками уже можно сразу просто брать готовые библиотеки и готовые датасеты и запускать всё это на видеокарте и в ус не дуть. Хотя это уже не так интересно.








